import pandas as pd
import numpy as np
from catboost import CatBoostClassifier, Pool
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, average_precision_score
import warnings

warnings.filterwarnings('ignore')

print("üöÄ –ó–ê–ü–£–°–ö –†–ï–ñ–ò–ú–ê 'GOD MODE' (ENSEMBLE + INTERACTION FEATURES)...")

# --- 1. –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ---
df_trans = pd.read_csv('data/—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏_–≤_–ú–æ–±–∏–ª—å–Ω–æ–º_–∏–Ω—Ç–µ—Ä–Ω–µ—Ç_–ë–∞–Ω–∫–∏–Ω–≥–µ.csv', sep=';', header=1, encoding='cp1251')
df_behav = pd.read_csv('data/–ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ_–ø–∞—Ç—Ç–µ—Ä–Ω—ã_–∫–ª–∏–µ–Ω—Ç–æ–≤_3.csv', sep=';', header=1, encoding='cp1251')

for df in [df_trans, df_behav]:
    df['transdate'] = pd.to_datetime(df['transdate'].astype(str).str.strip("'"))
df_trans['transdatetime'] = pd.to_datetime(df_trans['transdatetime'].astype(str).str.strip("'"))

df = pd.merge(df_trans, df_behav, on=['cst_dim_id', 'transdate'], how='left')

# --- 2. FEATURE ENGINEERING (MAXIMUM POWER) ---
print("üîß –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–ª–∏—Ç–Ω—ã—Ö —Ñ–∏—á–µ–π...")

# 2.1. User Context
user_stats = df.groupby('cst_dim_id')['amount'].agg(['mean', 'std', 'max']).reset_index()
user_stats.columns = ['cst_dim_id', 'user_mean', 'user_std', 'user_max']
df = pd.merge(df, user_stats, on='cst_dim_id', how='left')

df['amount_zscore'] = (df['amount'] - df['user_mean']) / (df['user_std'] + 1.0)
df['amount_to_max'] = df['amount'] / (df['user_max'] + 1.0) # –ù–∞—Å–∫–æ–ª—å–∫–æ –±–ª–∏–∑–∫–æ –∫ —Ä–µ–∫–æ—Ä–¥—É –∫–ª–∏–µ–Ω—Ç–∞
df['amount_log'] = np.log1p(df['amount'])

# 2.2. Time Features
df['hour'] = df['transdatetime'].dt.hour
df['is_night'] = df['hour'].apply(lambda x: 1 if x < 6 or x > 23 else 0)
df['day_of_week'] = df['transdatetime'].dt.dayofweek

# 2.3. Interaction Features (–ó–æ–ª–æ—Ç—ã–µ —Ñ–∏—á–∏)
# "–ù–æ—á–Ω–∞—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è –Ω–∞ –±–æ–ª—å—à—É—é —Å—É–º–º—É"
df['night_x_amount'] = df['is_night'] * df['amount_log']
# "–†–µ–¥–∫–∏–π –ø–æ–ª—É—á–∞—Ç–µ–ª—å + –ë–æ–ª—å—à–∞—è —Å—É–º–º–∞" (Frequency Encoding * Amount)
freq_map = df['direction'].value_counts(normalize=True).to_dict()
df['direction_freq'] = df['direction'].map(freq_map)
df['rare_high_amount'] = (1 - df['direction_freq']) * df['amount_log']

# --- 3. –ü–û–î–ì–û–¢–û–í–ö–ê ---
X_temp = df.drop(columns=['target'])
y_temp = df['target']
train_idx, test_idx = train_test_split(df.index, test_size=0.2, random_state=42, stratify=y_temp)

# 2.4. Target Encoding (Risk Score) - –û—Å—Ç–æ—Ä–æ–∂–Ω–æ, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å—Å—è
def smooth_target_encode(train_df, test_df, cat_col, target_col, weight=10):
    global_mean = train_df[target_col].mean()
    agg = train_df.groupby(cat_col)[target_col].agg(['count', 'mean'])
    counts = agg['count']
    means = agg['mean']
    smoothed = (counts * means + weight * global_mean) / (counts + weight)
    return train_df[cat_col].map(smoothed).fillna(global_mean), test_df[cat_col].map(smoothed).fillna(global_mean)

df.loc[train_idx, 'receiver_risk'], df.loc[test_idx, 'receiver_risk'] = \
    smooth_target_encode(df.loc[train_idx], df.loc[test_idx], 'direction', 'target', weight=5)

# NEW: Cross-Domain Interactions (Risk * Behavior)
# –¢–ï–ü–ï–†–¨, –∫–æ–≥–¥–∞ receiver_risk —Å–æ–∑–¥–∞–Ω, –º–æ–∂–Ω–æ –¥–µ–ª–∞—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ü–∏–∏
df['risk_x_zscore'] = df['receiver_risk'] * df['amount_zscore']
df['risk_x_night'] = df['receiver_risk'] * df['is_night']

# –û—á–∏—Å—Ç–∫–∞
drop_cols = ['cst_dim_id', 'transdate', 'transdatetime', 'docno', 'target',
             '–ó–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ–ª—É—á–∞—Ç–µ–ª—è/destination —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏', 
             'direction', 'user_mean', 'user_std', 'user_max']
X = df.drop(columns=[c for c in drop_cols if c in df.columns])
y = df['target']

# FillNA
for col in X.columns:
    if X[col].dtype == 'object':
        X[col] = X[col].fillna('Unknown')
    else:
        X[col] = X[col].fillna(0)

cat_features = [i for i, col in enumerate(X.columns) if X[col].dtype == 'object']

X_train = X.loc[train_idx]
y_train = y.loc[train_idx]
X_test = X.loc[test_idx]
y_test = y.loc[test_idx]

# --- 4. ENSEMBLE TRAINING (3 MODELS) ---
print("\nüî• –û–±—É—á–µ–Ω–∏–µ –ê–Ω—Å–∞–º–±–ª—è (3 –Ω–µ–π—Ä–æ—Å–µ—Ç–∏)...")

# Model 1: Precision Focused (All Features)
model_1 = CatBoostClassifier(
    iterations=1000, depth=4, l2_leaf_reg=10, learning_rate=0.05,
    auto_class_weights='SqrtBalanced', cat_features=cat_features, verbose=0, random_seed=42
)
model_1.fit(X_train, y_train)
print("‚úÖ Model 1 (Skeptic) ready.")

# Model 2: Recall Focused (NO RISK SCORE) - "–ò—â–µ–π–∫–∞"
# –£–±–∏—Ä–∞–µ–º receiver_risk, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –∏—Å–∫–∞–ª–∞ –∞–Ω–æ–º–∞–ª–∏–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è
X_train_no_risk = X_train.drop(columns=['receiver_risk'])
X_test_no_risk = X_test.drop(columns=['receiver_risk'])
cat_features_no_risk = [i for i, col in enumerate(X_train_no_risk.columns) if X_train_no_risk[col].dtype == 'object']

model_2 = CatBoostClassifier(
    iterations=1500, depth=10, l2_leaf_reg=0.1, learning_rate=0.01, # Aggressive Overfitting for Recall
    scale_pos_weight=20, # FORCE the model to find fraud
    cat_features=cat_features_no_risk, verbose=0, random_seed=43
)
model_2.fit(X_train_no_risk, y_train)
print("‚úÖ Model 2 (Bloodhound - No Risk) ready.")

# Model 3: Balanced (All Features)
model_3 = CatBoostClassifier(
    iterations=1000, depth=6, l2_leaf_reg=3, learning_rate=0.04,
    auto_class_weights='SqrtBalanced', cat_features=cat_features, verbose=0, random_seed=44
)
model_3.fit(X_train, y_train)
print("‚úÖ Model 3 (Realist) ready.")

# --- 5. VOTING (–ì–û–õ–û–°–û–í–ê–ù–ò–ï) ---
print("\nüó≥Ô∏è –ì–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")

p1 = model_1.predict_proba(X_test)[:, 1]
p2 = model_2.predict_proba(X_test_no_risk)[:, 1] # –í–∞–∂–Ω–æ: –ø–æ–¥–∞–µ–º X –±–µ–∑ —Ä–∏—Å–∫–∞
p3 = model_3.predict_proba(X_test)[:, 1]

# –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ
# 40% –°–∫–µ–ø—Ç–∏–∫ (–¢–æ—á–Ω–æ—Å—Ç—å), 40% –ò—â–µ–π–∫–∞ (–û—Ö–≤–∞—Ç), 20% –†–µ–∞–ª–∏—Å—Ç
final_proba = (p1 * 0.40) + (p2 * 0.40) + (p3 * 0.20)

# --- 6. –ü–û–ò–°–ö –ò–î–ï–ê–õ–¨–ù–û–ì–û –ë–ê–õ–ê–ù–°–ê ---
print("‚öñÔ∏è –ü–æ–¥–±–æ—Ä –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞...")
print(f"   Max Probability: {final_proba.max():.4f}")
print(f"   Mean Probability: {final_proba.mean():.4f}")

best_f1 = 0
best_thr = 0.5
best_metrics = {'Precision': 0, 'Recall': 0}

# –ò—â–µ–º –ø–æ—Ä–æ–≥, –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É—é—â–∏–π F1
for thr in np.arange(0.01, 0.95, 0.01):
    pred = (final_proba > thr).astype(int)
    
    prec = precision_score(y_test, pred, zero_division=0)
    rec = recall_score(y_test, pred)
    f1 = f1_score(y_test, pred)
    
    if f1 > best_f1:
        best_f1 = f1
        best_thr = thr
        best_metrics = {'Precision': prec, 'Recall': rec}

print("\n" + "="*40)
print(f"üèÜ –§–ò–ù–ê–õ–¨–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢ (ENSEMBLE)")
print("="*40)
print(f"Threshold: {best_thr:.2f}")
print(f"üíé Precision: {best_metrics['Precision']:.2%}")
print(f"üîç Recall:    {best_metrics['Recall']:.2%}")
print(f"‚öñÔ∏è F1-Score:  {best_f1:.2%}")
print("="*40)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å (–ø–æ —Ñ–∞–∫—Ç—É —Å–æ—Ö—Ä–∞–Ω–∏–º Model 3 –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω—É—é –¥–ª—è –¥–µ–º–æ, —Ç.–∫. –∞–Ω—Å–∞–º–±–ª—å —Å–ª–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª)
model_3.save_model("catboost_final.cbm")
print("\nüíæ –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'catboost_final.cbm'")
