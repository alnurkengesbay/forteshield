import pandas as pd
import numpy as np
from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score, precision_score, recall_score
from sklearn.ensemble import IsolationForest
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE, ADASYN
import warnings

warnings.filterwarnings('ignore')

print("üî• –ó–ê–ü–£–°–ö –†–ï–ñ–ò–ú–ê 'GOD MODE' (SMOTE + UNSUPERVISED LEARNING)...")

# --- 1. –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ---
df_trans = pd.read_csv('data/—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏_–≤_–ú–æ–±–∏–ª—å–Ω–æ–º_–∏–Ω—Ç–µ—Ä–Ω–µ—Ç_–ë–∞–Ω–∫–∏–Ω–≥–µ.csv', sep=';', header=1, encoding='cp1251')
df_behav = pd.read_csv('data/–ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ_–ø–∞—Ç—Ç–µ—Ä–Ω—ã_–∫–ª–∏–µ–Ω—Ç–æ–≤_3.csv', sep=';', header=1, encoding='cp1251')

for df in [df_trans, df_behav]:
    df['transdate'] = pd.to_datetime(df['transdate'].astype(str).str.strip("'"))
df_trans['transdatetime'] = pd.to_datetime(df_trans['transdatetime'].astype(str).str.strip("'"))

df = pd.merge(df_trans, df_behav, on=['cst_dim_id', 'transdate'], how='left')

# --- 2. FEATURE ENGINEERING (ALL-IN) ---
print("üîß –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏—á–µ–π (Supervised + Unsupervised)...")

# 2.1. User Context
user_stats = df.groupby('cst_dim_id')['amount'].agg(['mean', 'std', 'max']).reset_index()
user_stats.columns = ['cst_dim_id', 'user_mean', 'user_std', 'user_max']
df = pd.merge(df, user_stats, on='cst_dim_id', how='left')

df['amount_zscore'] = (df['amount'] - df['user_mean']) / (df['user_std'] + 1.0)
df['amount_to_max'] = df['amount'] / (df['user_max'] + 1.0)
df['amount_log'] = np.log1p(df['amount'])

# 2.2. Time Features
df['hour'] = df['transdatetime'].dt.hour
df['is_night'] = df['hour'].apply(lambda x: 1 if x < 6 or x > 23 else 0)

# 2.3. Frequency Features
freq_map = df['direction'].value_counts(normalize=True).to_dict()
df['direction_freq'] = df['direction'].map(freq_map)

# 2.4. UNSUPERVISED LEARNING FEATURES (The Secret Sauce)
# –ú—ã –¥–æ–±–∞–≤–ª—è–µ–º "–ú–Ω–µ–Ω–∏–µ" –¥—Ä—É–≥–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∫–∞–∫ —Ñ–∏—á–∏ –¥–ª—è CatBoost

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Unsupervised (—Ç–æ–ª—å–∫–æ —á–∏—Å–ª–µ–Ω–Ω—ã–µ)
num_cols = ['amount', 'amount_log', 'hour', 'direction_freq', 'user_mean']
X_unsup = df[num_cols].fillna(0)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_unsup)

# A. Isolation Forest (–ü–æ–∏—Å–∫ –∞–Ω–æ–º–∞–ª–∏–π)
print("   -> Running Isolation Forest...")
iso = IsolationForest(contamination=0.02, random_state=42)
df['iso_score'] = iso.fit_predict(X_scaled) # -1 for outlier, 1 for inlier
df['iso_anomaly'] = df['iso_score'].apply(lambda x: 1 if x == -1 else 0)

# B. K-Means Clustering (–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Ü–µ–Ω—Ç—Ä–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞)
print("   -> Running K-Means...")
kmeans = KMeans(n_clusters=5, random_state=42)
df['cluster'] = kmeans.fit_predict(X_scaled)
# –°—á–∏—Ç–∞–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Ü–µ–Ω—Ç—Ä–∞ —Å–≤–æ–µ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
centers = kmeans.cluster_centers_
df['dist_to_center'] = [np.linalg.norm(x - centers[c]) for x, c in zip(X_scaled, df['cluster'])]

# --- 3. –ü–û–î–ì–û–¢–û–í–ö–ê ---
X_temp = df.drop(columns=['target'])
y_temp = df['target']
train_idx, test_idx = train_test_split(df.index, test_size=0.2, random_state=42, stratify=y_temp)

# 2.5. Target Encoding (Risk Score)
def smooth_target_encode(train_df, test_df, cat_col, target_col, weight=10):
    global_mean = train_df[target_col].mean()
    agg = train_df.groupby(cat_col)[target_col].agg(['count', 'mean'])
    counts = agg['count']
    means = agg['mean']
    smoothed = (counts * means + weight * global_mean) / (counts + weight)
    return train_df[cat_col].map(smoothed).fillna(global_mean), test_df[cat_col].map(smoothed).fillna(global_mean)

df.loc[train_idx, 'receiver_risk'], df.loc[test_idx, 'receiver_risk'] = \
    smooth_target_encode(df.loc[train_idx], df.loc[test_idx], 'direction', 'target', weight=5)

# Interactions with Unsupervised Features
df['risk_x_iso'] = df['receiver_risk'] * df['iso_anomaly']
df['risk_x_dist'] = df['receiver_risk'] * df['dist_to_center']

# Cleanup
drop_cols = ['cst_dim_id', 'transdate', 'transdatetime', 'docno', 'target',
             '–ó–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ–ª—É—á–∞—Ç–µ–ª—è/destination —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏', 
             'direction', 'user_mean', 'user_std', 'user_max', 'iso_score']
X = df.drop(columns=[c for c in drop_cols if c in df.columns])
y = df['target']

# FillNA
for col in X.columns:
    if X[col].dtype == 'object':
        X[col] = X[col].fillna('Unknown')
    else:
        X[col] = X[col].fillna(0)

cat_features = [i for i, col in enumerate(X.columns) if X[col].dtype == 'object']
cat_features_names = [col for col in X.columns if X[col].dtype == 'object']

# Add discrete numeric columns to categorical features
discrete_cols = ['iso_anomaly', 'cluster', 'is_night', 'hour']
discrete_cols = [c for c in discrete_cols if c in X.columns]
cat_features_names.extend(discrete_cols)
cat_features_names = list(set(cat_features_names))

X_train = X.loc[train_idx]
y_train = y.loc[train_idx]
X_test = X.loc[test_idx]
y_test = y.loc[test_idx]

# --- 4. SMOTE (SYNTHETIC DATA GENERATION) ---
print("üß¨ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π —Ñ—Ä–æ–¥ (SMOTE)...")

# EXPERIMENT: Drop receiver_risk to force learning from behavior (Unsupervised Features)
print("   -> Dropping 'receiver_risk' to prevent overfitting to destination IDs...")
drop_risk_cols = ['receiver_risk', 'risk_x_iso', 'risk_x_dist']
X_train = X_train.drop(columns=[c for c in drop_risk_cols if c in X_train.columns])
X_test = X_test.drop(columns=[c for c in drop_risk_cols if c in X_test.columns])

# Update cat_features_names to exclude dropped columns if any
cat_features_names = [c for c in cat_features_names if c in X_train.columns]

# 1. –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
# –í–ê–ñ–ù–û: –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–∫–ª–∞—Å—Ç–µ—Ä—ã, —Ñ–ª–∞–≥–∏) –Ω–µ–ª—å–∑—è –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–æ–≤–∞—Ç—å SMOTE-–æ–º.
# –ò—Ö –Ω—É–∂–Ω–æ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
discrete_cols = ['iso_anomaly', 'cluster', 'is_night', 'hour']
discrete_cols = [c for c in discrete_cols if c in X_train.columns]

X_train_num = X_train.select_dtypes(include=[np.number]).drop(columns=discrete_cols)
X_train_cat = pd.concat([
    X_train.select_dtypes(exclude=[np.number]),
    X_train[discrete_cols]
], axis=1)

# 2. –ü—Ä–∏–º–µ–Ω—è–µ–º SMOTE —Ç–æ–ª—å–∫–æ –∫ –ù–ï–ü–†–ï–†–´–í–ù–´–ú —á–∏—Å–ª–∞–º
smote = SMOTE(random_state=42, k_neighbors=3, sampling_strategy=0.5) # –î–µ–ª–∞–µ–º —Ñ—Ä–æ–¥–∞ 50% –æ—Ç –Ω–æ—Ä–º—ã
X_res_num, y_res = smote.fit_resample(X_train_num, y_train)

# 3. –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (—á–µ—Ä–µ–∑ Random Sampling –∏–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Ñ—Ä–æ–¥–∞)
# –í–º–µ—Å—Ç–æ "Synthetic" (–∫–æ—Ç–æ—Ä—ã–π –ø–∞–ª–∏—Ç –∫–æ–Ω—Ç–æ—Ä—É), –º—ã –±–µ—Ä–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –æ—Ç –†–ï–ê–õ–¨–ù–´–• —Ñ—Ä–æ–¥–æ–≤—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π.

print("   -> Filling categorical features for synthetic data using Real Fraud samples...")
fraud_indices = y_train[y_train == 1].index
real_fraud_cats = X_train_cat.loc[fraud_indices]

# –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ–ª—å–∫–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä–æ–∫ –¥–æ–±–∞–≤–∏–ª SMOTE
n_synthetic = len(X_res_num) - len(X_train)

# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¢–û–õ–¨–ö–û –¥–ª—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–π —á–∞—Å—Ç–∏
synthetic_cats = real_fraud_cats.sample(n=n_synthetic, replace=True, random_state=42).reset_index(drop=True)

# –û–±—ä–µ–¥–∏–Ω—è–µ–º: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ + –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
X_train_cat_reset = X_train_cat.reset_index(drop=True)
X_res_cat_combined = pd.concat([X_train_cat_reset, synthetic_cats], axis=0).reset_index(drop=True)

# X_res_num —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ (–æ—Ç SMOTE)
X_res_num_combined = pd.DataFrame(X_res_num, columns=X_train_num.columns).reset_index(drop=True)

X_train_final = pd.concat([X_res_num_combined, X_res_cat_combined], axis=1)
y_train_final = y_res

# –í–ê–ñ–ù–û: –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫!
# SMOTE —Ä–∞–∑–¥–µ–ª–∏–ª –∏—Ö –Ω–∞ num –∏ cat, –∏ concat —Å–∫–ª–µ–∏–ª –∏—Ö –≤ –∫—É—á—É.
# –ü–æ—Ä—è–¥–æ–∫ –≤ X_train_final —Å–µ–π—á–∞—Å: [–í—Å–µ —á–∏—Å–ª–∞, –í—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏]
# –ê –≤ X_test –ø–æ—Ä—è–¥–æ–∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π. –≠—Ç–æ –ª–æ–º–∞–µ—Ç CatBoost (Feature #3 mismatch).
X_train_final = X_train_final[X_test.columns]

# –í–ê–ñ–ù–û: –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∫ —Å—Ç—Ä–æ–∫–∞–º, —á—Ç–æ–±—ã CatBoost –Ω–µ —Ä—É–≥–∞–ª—Å—è –Ω–∞ float
# –ò —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –Ω–µ—Ç float-–ø–æ–¥–æ–±–Ω—ã—Ö —Å—Ç—Ä–æ–∫ —Ç–∏–ø–∞ "11.0"
for col in X_train_final.columns:
    if col in cat_features_names:
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤ int, –ø–æ—Ç–æ–º –≤ str, —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å .0
        # –ï—Å–ª–∏ —Ç–∞–º –µ—Å—Ç—å 'Unknown' –∏–ª–∏ 'Synthetic', —Ç–æ try-except
        def clean_cat(x):
            try:
                return str(int(float(x)))
            except:
                return str(x)
        
        X_train_final[col] = X_train_final[col].apply(clean_cat)
        X_test[col] = X_test[col].apply(clean_cat)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∫–∞–∫ object/string
        X_train_final[col] = X_train_final[col].apply(clean_cat).astype(str)
        X_test[col] = X_test[col].apply(clean_cat).astype(str)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤
print("–¢–∏–ø—ã –∫–æ–ª–æ–Ω–æ–∫ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏:")
print(X_train_final.dtypes)

# –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ cat_features - —ç—Ç–æ –∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫, –∞ –Ω–µ –∏–Ω–¥–µ–∫—Å—ã, —Ç–∞–∫ –∫–∞–∫ –º—ã –ø–µ—Ä–µ–¥–∞–µ–º DataFrame
cat_features_names = [col for col in X_train_final.columns if X_train_final[col].dtype == 'object']
print(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏: {cat_features_names}")

# –í–ê–ñ–ù–û: CatBoost –∏–Ω–æ–≥–¥–∞ –ø—É—Ç–∞–µ—Ç—Å—è, –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å DataFrame —Å object –∫–æ–ª–æ–Ω–∫–∞–º–∏, –Ω–æ –Ω–µ —É–∫–∞–∑—ã–≤–∞—Ç—å –∏—Ö –∫–∞–∫ cat_features
# –ò–ª–∏ –µ—Å–ª–∏ —É–∫–∞–∑—ã–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å—ã, –Ω–æ –æ–Ω –æ–∂–∏–¥–∞–µ—Ç –∏–º–µ–Ω–∞.
# –°–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π —Å–ø–æ—Å–æ–±: –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ object –≤ 'category' —Ç–∏–ø pandas.

# –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –í–°–ï –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ 'category'
# –ò –≤ Train, –∏ –≤ Test, —á—Ç–æ–±—ã —Ç–∏–ø—ã —Å–æ–≤–ø–∞–¥–∞–ª–∏
for col in cat_features_names:
    X_train_final[col] = X_train_final[col].astype('category')
    X_test[col] = X_test[col].astype('category')

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤
print("–¢–∏–ø—ã –∫–æ–ª–æ–Ω–æ–∫ –ø–æ—Å–ª–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ category:")
print(X_train_final.dtypes)

print(f"   -> –ë—ã–ª–æ —Ñ—Ä–æ–¥–∞: {y_train.sum()}, –°—Ç–∞–ª–æ: {y_train_final.sum()}")

# --- 5. TRAINING ---
# –í–ê–ñ–ù–û: CatBoost —Ç—Ä–µ–±—É–µ—Ç, —á—Ç–æ–±—ã –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –≤ eval_set —Å–æ–≤–ø–∞–¥–∞–ª —Å X_train
# –ò —á—Ç–æ–±—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏ –±—ã–ª–∏ —É–∫–∞–∑–∞–Ω—ã —è–≤–Ω–æ.

# –í–ê–ñ–ù–û: –°–æ–∑–¥–∞–µ–º Pool –æ–±—ä–µ–∫—Ç—ã —è–≤–Ω–æ, —á—Ç–æ–±—ã CatBoost —Ç–æ—á–Ω–æ –∑–Ω–∞–ª, –≥–¥–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
from catboost import Pool
train_pool = Pool(X_train_final, y_train_final, cat_features=cat_features_names) # –ò–º–µ–Ω–∞ –Ω–∞–¥–µ–∂–Ω–µ–µ —Å pandas
test_pool = Pool(X_test, y_test, cat_features=cat_features_names)

model = CatBoostClassifier(
    iterations=2000,
    learning_rate=0.03,
    depth=6,
    l2_leaf_reg=3,
    auto_class_weights='Balanced', 
    verbose=200,
    early_stopping_rounds=200,
    eval_metric='F1',
    random_seed=42
)

model.fit(train_pool, eval_set=test_pool)

# --- 6. THRESHOLD SEARCH (BRUTE FORCE) ---
print("\nüïµÔ∏è –ò—â–µ–º '–ó–æ–ª–æ—Ç–æ–µ —Å–µ—á–µ–Ω–∏–µ' (80/80)...")
y_prob = model.predict_proba(X_test)[:, 1]

best_thr = 0.5
best_score = 0
final_metrics = {}

# –ú—ã –∏—â–µ–º —Ç–æ—á–∫—É, –≥–¥–µ (Precision + Recall) –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ, –ù–û –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏ —á—Ç–æ –æ–±–∞ > 0.7
print(f"{'Thr':<5} | {'Prec':<8} | {'Recall':<8} | {'Score':<8}")
print("-" * 35)

for thr in np.arange(0.05, 0.95, 0.05):
    pred = (y_prob > thr).astype(int)
    p = precision_score(y_test, pred, zero_division=0)
    r = recall_score(y_test, pred)
    
    # –ù–∞—à–∞ —Ü–µ–ª—å: –û–±–∞ > 0.8. –ï—Å–ª–∏ –Ω–µ—Ç, —Ç–æ —Ö–æ—Ç—è –±—ã –æ–±–∞ > 0.75
    score = p + r
    
    print(f"{thr:<5.2f} | {p:<8.2%} | {r:<8.2%} | {score:<8.2f}")

    # –®—Ç—Ä–∞—Ñ—É–µ–º, –µ—Å–ª–∏ –æ–¥–∏–Ω –∏–∑ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –Ω–∏–∑–∫–∏–π
    if p < 0.7 or r < 0.7:
        score = score * 0.5 
        
    if score > best_score:
        best_score = score
        best_thr = thr
        final_metrics = {'Precision': p, 'Recall': r}

print("\n" + "="*40)
print(f"üèÜ GOD MODE RESULT")
print("="*40)
print(f"Threshold: {best_thr:.2f}")
print(f"üíé Precision: {final_metrics['Precision']:.2%}")
print(f"üîç Recall:    {final_metrics['Recall']:.2%}")
print("="*40)

model.save_model("catboost_god.cbm")
